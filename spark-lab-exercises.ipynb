{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark SQL Using Apache Hadoop HDFS\n",
    "Click [here (Medium)](https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-242445b24565) for source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row, Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import hashlib\n",
    "# import pyspark-stubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"500mb\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Read the source tables in Parquet format\n",
    "products_table = spark.read.parquet(\"./data/products_parquet\")\n",
    "sales_table = spark.read.parquet(\"./data/sales_parquet\")\n",
    "sellers_table = spark.read.parquet(\"./data/sellers_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(product_id='0', product_name='product_0', price='22'),\n",
       " Row(product_id='1', product_name='product_1', price='30'),\n",
       " Row(product_id='2', product_name='product_2', price='91'),\n",
       " Row(product_id='3', product_name='product_3', price='37'),\n",
       " Row(product_id='4', product_name='product_4', price='145')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product_id', 'product_name', 'price']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(order_id='1', product_id='0', seller_id='0', date='2020-07-10', num_pieces_sold='26', bill_raw_text='kyeibuumwlyhuwksxodcawelubbyznxvpfxyxzhftudwtiemdhqaqoDeodltimrusmwtfdikxoqpuFliyvacNgrpigspylwaxvbdwiqurqkqidkujwziucglsvzxjbswtkeogxyncoweuvczualykpszcbrmwwyjkgbvpfyijshnkbjdrxppwmclymvofjslldhwnwajnnyvzktbjixlsodivgmfsvwjfqjuwwrqelkkzupmlggzeqmcskkvjggnajgimzkdtzgvhuzlgubkigdctumjhdwnzzlnrlyuhlhmlgfxuhyxlzraqhqgfsgvtigkfuuuawoxherqhovyyyglsxwoqkwntuaesstxvtwhzvzxincwpkhigcwslpokwplgrotrnttohkzzumqcejkvgwkvexpnyjrwzbfauervihrvezyfndmekllabllewmqrgbfwimygcwlbeblcfqitaxnoaudfgaodqbmsgavetmrtlnyy')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'product_id',\n",
       " 'seller_id',\n",
       " 'date',\n",
       " 'num_pieces_sold',\n",
       " 'bill_raw_text']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(seller_id='0', seller_name='seller_0', daily_target='2500000'),\n",
       " Row(seller_id='1', seller_name='seller_1', daily_target='257237'),\n",
       " Row(seller_id='2', seller_name='seller_2', daily_target='754188'),\n",
       " Row(seller_id='3', seller_name='seller_3', daily_target='310462'),\n",
       " Row(seller_id='4', seller_name='seller_4', daily_target='1532808')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sellers_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seller_id', 'seller_name', 'daily_target']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sellers_table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up #1\n",
    "Find out how many orders, how many products and how many sellers are in the data.\n",
    "How many products have been sold at least once? Which is the product contained in more orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Orders: 20000040\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of orders\n",
    "print(\"Number of Orders: {}\".format(sales_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sellers: 10\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of sellers\n",
    "print(\"Number of sellers: {}\".format(sellers_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products: 75000000\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of products\n",
    "print(\"Number of products: {}\".format(products_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products sold at least once\n",
      "+--------------------------+\n",
      "|count(DISTINCT product_id)|\n",
      "+--------------------------+\n",
      "|                    993429|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Output how many products have been actually sold at least once\n",
    "print(\"Number of products sold at least once\")\n",
    "sales_table.agg(countDistinct(col(\"product_id\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product present in more orders\n",
      "+----------+--------+\n",
      "|product_id|     cnt|\n",
      "+----------+--------+\n",
      "|         0|19000000|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Output which is the product that has been sold in more orders\n",
    "print(\"Product present in more orders\")\n",
    "sales_table.groupBy(col(\"product_id\")).agg(\n",
    "    count(\"*\").alias(\"cnt\")).orderBy(col(\"cnt\").desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up #2\n",
    "How many distinct products have been sold in each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      date|distinct_products_sold|\n",
      "+----------+----------------------+\n",
      "|2020-07-06|                100765|\n",
      "|2020-07-09|                100501|\n",
      "|2020-07-01|                100337|\n",
      "|2020-07-03|                100017|\n",
      "|2020-07-02|                 99807|\n",
      "|2020-07-05|                 99796|\n",
      "|2020-07-04|                 99791|\n",
      "|2020-07-07|                 99756|\n",
      "|2020-07-08|                 99662|\n",
      "|2020-07-10|                 98973|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_table.groupby(col(\"date\")).agg(countDistinct(col(\"product_id\")).alias(\"distinct_products_sold\")).orderBy(\n",
    "    col(\"distinct_products_sold\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #1\n",
    "What is the average revenue of the orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"500mb\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Read the source tables in Parquet format\n",
    "products_table = spark.read.parquet(\"./data/products_parquet\")\n",
    "sales_table = spark.read.parquet(\"./data/sales_parquet\")\n",
    "sellers_table = spark.read.parquet(\"./data/sellers_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Wrong way to implement - do not run the cell (skewed)\n",
    "print(sales_table.join(products_table, sales_table[\"product_id\"] == products_table[\"product_id\"], \"inner\").\n",
    "      agg(avg(products_table[\"price\"] * sales_table[\"num_pieces_sold\"])).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Check and select the skewed keys \n",
    "results = sales_table.groupby(sales_table[\"product_id\"]).count().sort(col(\"count\").desc()).limit(100).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - What we want to do is:\n",
    "#  a. Duplicate the entries that we have in the dimension table for the most common products, e.g.\n",
    "#       product_0 will become: product_0-1, product_0-2, product_0-3 and so on\n",
    "#  b. On the sales table, we are going to replace \"product_0\" with a random duplicate (e.g. some of them \n",
    "#     will be replaced with product_0-1, others with product_0-2, etc.)\n",
    "# Using the new \"salted\" key will unskew the join\n",
    "\n",
    "# Let's create a dataset to do the trick\n",
    "REPLICATION_FACTOR = 101\n",
    "l = []\n",
    "replicated_products = []\n",
    "for _r in results:\n",
    "    replicated_products.append(_r[\"product_id\"])\n",
    "    for _rep in range(0, REPLICATION_FACTOR):\n",
    "        l.append((_r[\"product_id\"], _rep))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "replicated_df = rdd.map(lambda x: Row(product_id=x[0], replication=int(x[1])))\n",
    "replicated_df = spark.createDataFrame(replicated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products Table Schema:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n",
      "Replicated DataFrame Schema:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- replication: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Products Table Schema:\")\n",
    "products_table.printSchema()\n",
    "\n",
    "print(\"Replicated DataFrame Schema:\")\n",
    "replicated_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Step 3: Generate the salted key\n",
    "products_table = products_table.join(broadcast(replicated_df),\n",
    "                                     products_table[\"product_id\"] == replicated_df[\"product_id\"], \"left\"). \\\n",
    "    withColumn(\"salted_join_key\", when(replicated_df[\"replication\"].isNull(), products_table[\"product_id\"]).otherwise(\n",
    "    concat(replicated_df[\"product_id\"], lit(\"-\"), replicated_df[\"replication\"])))\n",
    "\n",
    "sales_table = sales_table.withColumn(\"salted_join_key\", when(sales_table[\"product_id\"].isin(replicated_products),\n",
    "                                                             concat(sales_table[\"product_id\"], lit(\"-\"),\n",
    "                                                                    round(rand() * (REPLICATION_FACTOR - 1), 0).cast(\n",
    "                                                                        IntegerType()))).otherwise(\n",
    "    sales_table[\"product_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|            1246.1338560822878|\n",
      "+------------------------------+\n",
      "\n",
      "None\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "#   Step 4: Finally let's do the join\n",
    "print(sales_table.join(products_table, sales_table[\"salted_join_key\"] == products_table[\"salted_join_key\"],\n",
    "                       \"inner\").\n",
    "      agg(avg(products_table[\"price\"] * sales_table[\"num_pieces_sold\"])).show())\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #2\n",
    "For each seller, what is the average % contribution of an order to the seller's daily quota?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the source tables\n",
    "products_table = spark.read.parquet(\"./data/products_parquet\")\n",
    "sales_table = spark.read.parquet(\"./data/sales_parquet\")\n",
    "sellers_table = spark.read.parquet(\"./data/sellers_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|seller_id|          avg(ratio)|\n",
      "+---------+--------------------+\n",
      "|        0|2.019885898946922...|\n",
      "|        7|2.595228787788170...|\n",
      "|        3| 1.62888537056594E-4|\n",
      "|        8|9.213030375408861E-5|\n",
      "|        5|4.211073965904022E-5|\n",
      "|        6|4.782147194369122E-5|\n",
      "|        9|3.837913136180238E-5|\n",
      "|        1|1.964233366461014...|\n",
      "|        4|3.296428039825817E-5|\n",
      "|        2|6.690408001060484E-5|\n",
      "+---------+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sales_table.join(broadcast(sellers_table), sales_table[\"seller_id\"] == sellers_table[\"seller_id\"], \"inner\").withColumn(\n",
    "    \"ratio\", sales_table[\"num_pieces_sold\"]/sellers_table[\"daily_target\"]\n",
    ").groupBy(sales_table[\"seller_id\"]).agg(avg(\"ratio\")).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #3\n",
    "Who are the second most selling and the least selling persons (sellers) for each product? Who are those for product with `product_id = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the source tables\n",
    "products_table = spark.read.parquet(\"./data/products_parquet\")\n",
    "sales_table = spark.read.parquet(\"./data/sales_parquet\")\n",
    "sellers_table = spark.read.parquet(\"./data/sellers_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate the number of pieces sold by each seller for each product\n",
    "sales_table = sales_table.groupby(col(\"product_id\"), col(\"seller_id\")). \\\n",
    "    agg(sum(\"num_pieces_sold\").alias(\"num_pieces_sold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the window functions, one will sort ascending the other one descending. Partition by the product_id\n",
    "# and sort by the pieces sold\n",
    "window_desc = Window.partitionBy(col(\"product_id\")).orderBy(col(\"num_pieces_sold\").desc())\n",
    "window_asc = Window.partitionBy(col(\"product_id\")).orderBy(col(\"num_pieces_sold\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dense Rank (to avoid holes)\n",
    "sales_table = sales_table.withColumn(\"rank_asc\", dense_rank().over(window_asc)). \\\n",
    "    withColumn(\"rank_desc\", dense_rank().over(window_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get products that only have one row OR the products in which multiple sellers sold the same amount\n",
    "# (i.e. all the employees that ever sold the product, sold the same exact amount)\n",
    "single_seller = sales_table.where(col(\"rank_asc\") == col(\"rank_desc\")).select(\n",
    "    col(\"product_id\").alias(\"single_seller_product_id\"), col(\"seller_id\").alias(\"single_seller_seller_id\"),\n",
    "    lit(\"Only seller or multiple sellers with the same results\").alias(\"type\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the second top sellers\n",
    "second_seller = sales_table.where(col(\"rank_desc\") == 2).select(\n",
    "    col(\"product_id\").alias(\"second_seller_product_id\"), col(\"seller_id\").alias(\"second_seller_seller_id\"),\n",
    "    lit(\"Second top seller\").alias(\"type\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the least sellers and exclude those rows that are already included in the first piece\n",
    "# We also exclude the \"second top sellers\" that are also \"least sellers\"\n",
    "least_seller = sales_table.where(col(\"rank_asc\") == 1).select(\n",
    "    col(\"product_id\"), col(\"seller_id\"),\n",
    "    lit(\"Least Seller\").alias(\"type\")\n",
    ").join(single_seller, (sales_table[\"seller_id\"] == single_seller[\"single_seller_seller_id\"]) & (\n",
    "        sales_table[\"product_id\"] == single_seller[\"single_seller_product_id\"]), \"left_anti\"). \\\n",
    "    join(second_seller, (sales_table[\"seller_id\"] == second_seller[\"second_seller_seller_id\"]) & (\n",
    "        sales_table[\"product_id\"] == second_seller[\"second_seller_product_id\"]), \"left_anti\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|product_id|seller_id|        type|\n",
      "+----------+---------+------------+\n",
      "|  19986717|        1|Least Seller|\n",
      "|  72017876|        1|Least Seller|\n",
      "|   3534470|        3|Least Seller|\n",
      "|  35669461|        4|Least Seller|\n",
      "|  14542470|        5|Least Seller|\n",
      "|  28592106|        5|Least Seller|\n",
      "|  34681047|        5|Least Seller|\n",
      "|  40496308|        5|Least Seller|\n",
      "|  56011040|        5|Least Seller|\n",
      "|  67723231|        5|Least Seller|\n",
      "|  69790381|        5|Least Seller|\n",
      "|  10978356|        7|Least Seller|\n",
      "|  18182299|        7|Least Seller|\n",
      "|  52606213|        7|Least Seller|\n",
      "|  61475460|        7|Least Seller|\n",
      "|  17944574|        8|Least Seller|\n",
      "|  36269838|        8|Least Seller|\n",
      "|  20774718|        9|Least Seller|\n",
      "|  31136332|        9|Least Seller|\n",
      "|  32602520|        9|Least Seller|\n",
      "+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union all the pieces\n",
    "union_table = least_seller.select(\n",
    "    col(\"product_id\"),\n",
    "    col(\"seller_id\"),\n",
    "    col(\"type\")\n",
    ").union(second_seller.select(\n",
    "    col(\"second_seller_product_id\").alias(\"product_id\"),\n",
    "    col(\"second_seller_seller_id\").alias(\"seller_id\"),\n",
    "    col(\"type\")\n",
    ")).union(single_seller.select(\n",
    "    col(\"single_seller_product_id\").alias(\"product_id\"),\n",
    "    col(\"single_seller_seller_id\").alias(\"seller_id\"),\n",
    "    col(\"type\")\n",
    "))\n",
    "union_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|product_id|seller_id|                type|\n",
      "+----------+---------+--------------------+\n",
      "|         0|        0|Only seller or mu...|\n",
      "+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which are the second top seller and least seller of product 0?\n",
    "union_table.where(col(\"product_id\") == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #4\n",
    "Create a new column called \"hashed_bill\" defined as follows:\n",
    "- if the order_id is even: apply MD5 hashing iteratively to the bill_raw_text field, once for each 'A' (capital 'A') present in the text. E.g. if the bill text is 'nbAAnllA', you would apply hashing three times iteratively (only if the order number is even)\n",
    "- if the order_id is odd: apply SHA256 hashing to the bill text\n",
    "Finally, check if there are any duplicate on the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Init spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Load source data\n",
    "products_table = spark.read.parquet(\"./data/products_parquet\")\n",
    "sales_table = spark.read.parquet(\"./data/sales_parquet\")\n",
    "sellers_table = spark.read.parquet(\"./data/sellers_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Define the UDF function\n",
    "def algo(order_id, bill_text):\n",
    "    #   If number is even\n",
    "    ret = bill_text.encode(\"utf-8\")\n",
    "    if int(order_id) % 2 == 0:\n",
    "        #   Count number of 'A'\n",
    "        cnt_A = bill_text.count(\"A\")\n",
    "        for _c in range(0, cnt_A):\n",
    "            ret = hashlib.md5(ret).hexdigest().encode(\"utf-8\")\n",
    "        ret = ret.decode('utf-8')\n",
    "    else:\n",
    "        ret = hashlib.sha256(ret).hexdigest()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Register the UDF function.\n",
    "algo_udf = spark.udf.register(\"algo\", algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|hashed_bill|cnt|\n",
      "+-----------+---+\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Use the `algo_udf` to apply the aglorithm and then check if there is any duplicate hash in the table\n",
    "sales_table.withColumn(\"hashed_bill\", algo_udf(col(\"order_id\"), col(\"bill_raw_text\")))\\\n",
    "    .groupby(col(\"hashed_bill\")).agg(count(\"*\").alias(\"cnt\")).where(col(\"cnt\") > 1).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
